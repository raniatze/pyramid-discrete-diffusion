# Experiment settings
prev_stage: 's_1'  # choices: none, s_1, s_2
next_stage: 's_2'  # choices: s_1, s_2, s_3
resume: true
resume_path: '/home/raniatze/Documents/PhD/Research/pyramid-discrete-diffusion/checkpoints/s_1_to_s_2/models/epoch999.tar'
generation_num: 10
mode: 'inference' # choices: train, inference, infinity_gen
infer_data_source: 'generation' # choices: dataset, generation ### TODO: Change to 'generation' if you want to use the generated data from s_1

# Scene settings
prev_scene_path: '/home/raniatze/Documents/PhD/Research/pyramid-discrete-diffusion/generated/s_1/Generated/' ### TODO: If you set 'infer_data_source' as 'generation', set the path to the generated scene file from s_1.
scene_fusion_method: 'discard'  # choices: discard, force, vote
mask_ratio: 0.0625
infinite_ratio: 0.1875
mask_prob: [0.25, 0.25, 0.25, 0.25]  # [0] for up, [1] for left, [2] for up and left, [3] for none
infinity_size: [3, 2]

# Data settings
dataset: 'pritti'  # choices: carla, pritti, kitti
train_data_path: ''
quantized_train_data_path: ''
infer_data_path: '/home/raniatze/Documents/skitti_workspace/cache/pdd_cache/voxel_cache_64'
quantized_infer_data_path: '/home/raniatze/Documents/skitti_workspace/cache/pdd_cache/voxel_cache_32'
data_argumentation: false

# Hardware and distribution settings
gpu: 0
distribution: false
num_node: 1
node_rank: 0
dist_url: 'tcp://localhost:29500'

# Training parameters
batch_size: 1
num_workers: 8
pin_memory: false
epochs: 1000
check_every: 100

# Loss and optimization settings
clip_value: null
clip_norm: null
recon_loss: false
auxiliary_loss_weight: 0.0005
optimizer: 'adamw'  # choices: adamw, sgd, etc.
lr: 0.004
warmup: null
momentum: 0.9
momentum_sqr: 0.999
milestones: []
gamma: 0.1

# Model settings
model_type: 'con'  # choices: uncon, con, l_vae, l_gen
diffusion_steps: 100
diffusion_dim: 32
dp_rate: 0.0
l_size: '32322'  # choices: 882, 16162, 32322
init_size: 8
l_attention: true
vq_size: 50
vqvae_path: ''

# Logging and paths
log_home: null
exp_name: 'default'